{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizer\n",
    "### Use this script to visualize the neuron traces according to the paper \"A Generalized Linear Integrate-and-Fire Neural Model Produces Diverse Spiking Behaviors\" by Stefan Mihalas and Ernst Niebur. Further, data was created with a fix length of 1sec (1ms time steps), with noise on the input current, and/or temporal jitter on the time point of the step for dynamic inputs. \n",
    "\n",
    "### The script will also calculate the inter-spike intervalls (ISIs) for a single trial and for all repeating trials, whenever possible. For repeating trials, all ISIs are grouped and further statics represent the outcome of all repetitions per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.polynomial.polynomial as poly\n",
    "\n",
    "from tactile_encoding.utils.utils import value2key, create_directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_trials = 100\n",
    "\n",
    "classes_list = {\n",
    "    'A': \"Tonic spiking\",\n",
    "    'B': \"Class 1\",\n",
    "    'C': \"Spike frequency adaptation\",\n",
    "    'D': \"Phasic spiking\",\n",
    "    'E': \"Accommodation\",\n",
    "    'F': \"Threshold variability\",\n",
    "    'G': \"Rebound spike\",\n",
    "    'H': \"Class 2\",\n",
    "    'I': \"Integrator\",\n",
    "    'J': \"Input bistability\",\n",
    "    'K': \"Hyperpolarizing spiking\",\n",
    "    'L': \"Hyperpolarizing bursting\",\n",
    "    'M': \"Tonic bursting\",\n",
    "    'N': \"Phasic bursting\",\n",
    "    'O': \"Rebound burst\",\n",
    "    'P': \"Mixed mode\",\n",
    "    'Q': \"Afterpotentials\",\n",
    "    'R': \"Basal bistability\",\n",
    "    'S': \"Preferred frequency\",\n",
    "    'T': \"Spike latency\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_isi_fix_len(data, max_trials, norm_count=False, norm_time=False):\n",
    "    \"\"\"\n",
    "    Calculates and returns the ISI for all repetitions of fix length data.\n",
    "    \"\"\"\n",
    "    isi_list = []\n",
    "    for num, el in enumerate(list(classes_list.values())):\n",
    "        # print(el)\n",
    "        # concatenate all ISIs\n",
    "        isi_fix_len = []\n",
    "        for trial in range(max_trials):\n",
    "            # calc spikes per trial\n",
    "            spikes = np.reshape(np.array(data[trial + num*max_trials][0]), (np.array(\n",
    "                data[trial + num*max_trials][0]).shape[0]))\n",
    "            # calc ISI\n",
    "            isi_fix_len.extend(np.diff(np.where(spikes == 1)[0]))\n",
    "\n",
    "        if len(isi_fix_len) > 0:\n",
    "            tmp_fix_len = np.unique(isi_fix_len, return_counts=True)\n",
    "            isi_fix_len = tmp_fix_len[0]\n",
    "            if norm_time:\n",
    "                isi_fix_len = isi_fix_len/max(isi_fix_len)\n",
    "            isi_fix_len_count = tmp_fix_len[1]\n",
    "            if norm_count:\n",
    "                isi_fix_len_count = isi_fix_len_count/max(isi_fix_len_count)\n",
    "            # create 2d array\n",
    "            isi = np.vstack([isi_fix_len, isi_fix_len_count])\n",
    "        isi_list.append(isi)\n",
    "\n",
    "    return isi_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NTE - Tactile encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path_braille = './data/braille_mn_output'  # path to load data\n",
    "data_path_original = './data/original_mn_output'\n",
    "plot_out = './plots'\n",
    "# neuron parameters\n",
    "a: 2.743\n",
    "A1: 0.03712\n",
    "A2: -0.5089\n",
    "b: 11.4\n",
    "G: 47.02\n",
    "k1: 200\n",
    "k2: 20\n",
    "R1: 0\n",
    "R2: 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO check if ALL data from batches are extracted and extented -> final numper of ISI is too low\n",
    "file_names = os.listdir(data_path_braille)\n",
    "file_names = np.sort(file_names)\n",
    "isi_list = []\n",
    "isi_dict = {}\n",
    "for entry in range(27):\n",
    "    isi_dict[entry] = []\n",
    "for _, file_name in enumerate(file_names):\n",
    "    # print(file_name)\n",
    "    # only look at test\n",
    "    if 'test' in file_name:\n",
    "        [mn_spk, input_current, trial_label] = torch.load(\n",
    "            data_path_braille + '/' + file_name, map_location=torch.device('cpu'))\n",
    "        # convert to numpy\n",
    "        mn_spk = mn_spk.numpy()\n",
    "        input_current = input_current.numpy()\n",
    "        trial_label = trial_label.numpy()\n",
    "\n",
    "        # mn_spk.shape: [batch_size, time_steps, channels]\n",
    "        # print(mn_spk.shape)\n",
    "        # print(len(trial_label))  # one label per batch\n",
    "        # print(np.unique(trial_label))\n",
    "        # isi_dict = {}\n",
    "        # for _, entry in enumerate(np.unique(trial_label)):\n",
    "        #     isi_dict[entry] = []\n",
    "        # print(isi_dict[0])\n",
    "        # extract traces from single batch\n",
    "        for batch in range(mn_spk.shape[0]):\n",
    "            # print('Batch: ', batch)\n",
    "            # TODO find all trials with same label and concatenate ISIs!!!!!\n",
    "            # print('Trial label: ', trial_label[batch])\n",
    "            # print('Data shape in batch: ', mn_spk[batch].shape)\n",
    "            # print('Number of spikes per channel over time: ',\n",
    "            #       np.sum(mn_spk[batch], axis=0))  # sum along time\n",
    "            # print('Total number of spikes in batch: ',\n",
    "            #       np.sum(mn_spk[batch]))  # total sum\n",
    "\n",
    "            isi_list = []\n",
    "            # channels_list = np.where(np.sum(mn_spk[batch], axis=0) > 1)\n",
    "            # print('Channel list: ', channels_list)\n",
    "            # for _, channel in enumerate(channels_list[0]):\n",
    "            # loop over all channels\n",
    "            # print(mn_spk[batch].shape)\n",
    "            for channel in range(mn_spk[batch].shape[-1]):\n",
    "                # print('Channel: ', channel)\n",
    "                # print('Nbr spikes per hannel: ', sum(mn_spk[batch][:, channel]))\n",
    "                # print('Spike samples: ', np.where(mn_spk[batch][:, channel]==1.0))\n",
    "                # print('ISIs: ', np.diff(np.where(mn_spk[batch][:, channel]==1.0))*1E-2)\n",
    "                # calc ISI if at least 2 spikes found\n",
    "                if np.sum(mn_spk[batch][:, channel]) > 1:\n",
    "                    # print('Calc ISI. Spikecount: ', np.sum(mn_spk[batch][:, channel]))\n",
    "                    isi_channel = np.diff(\n",
    "                        np.where(mn_spk[batch][:, channel] == 1.0))*1E-2\n",
    "\n",
    "                    # count ISIs and normalize in count and time\n",
    "                    # returns member and count\n",
    "                    unique_isi = np.unique(isi_channel, return_counts=True)\n",
    "                    isi_braille = unique_isi[0]\n",
    "                    # if norm_time:\n",
    "                    #     isi_braille = isi_braille/max(isi_braille)\n",
    "                    isi_braille_count = unique_isi[1]\n",
    "                    # if norm_count:\n",
    "                    #     isi_braille_count = isi_braille_count / \\\n",
    "                    #         max(isi_braille_count)\n",
    "                    isi = np.vstack([isi_braille, isi_braille_count])\n",
    "                else:\n",
    "                    # print('Spikecount < 2.')\n",
    "                    isi = [[], []]\n",
    "                isi_list.append(isi)\n",
    "                # print(isi, isi_braille, isi_braille_count)\n",
    "            # print('ISI dict before:', isi_dict[trial_label[batch]])\n",
    "            # print('len dict before: ', len(isi_dict[trial_label[batch]]))\n",
    "\n",
    "            # init dict per label\n",
    "            if len(isi_dict[trial_label[batch]]) < 1:\n",
    "                # working\n",
    "                # print('INIT')\n",
    "                isi_dict[trial_label[batch]] = isi_list\n",
    "            # extend dict\n",
    "            else:\n",
    "                # print('EXTEND')\n",
    "                # extend the dict for each channel\n",
    "                for entry in range(len(isi_dict[trial_label[batch]])):\n",
    "                    # skip if nothing\n",
    "                    # if len(isi_dict[trial_label[batch]][entry][0]) > 1.0:\n",
    "                        # print('isi_dict[trial_label[batch]][entry]: ', isi_dict[trial_label[batch]][entry])\n",
    "                        # print('len(isi_dict[trial_label[batch]][entry]): ', len(isi_dict[trial_label[batch]][entry]))\n",
    "                        # print('len(isi_dict[trial_label[batch]][entry][0]): ', len(isi_dict[trial_label[batch]][entry][0]))\n",
    "                        \n",
    "                        # print('isi_list[entry]: ', isi_list[entry])\n",
    "                        # print('len(isi_list[entry]): ', len(isi_list[entry]))\n",
    "                        # print('len(isi_list[entry][0]): ', len(isi_list[entry][0]))\n",
    "\n",
    "                    isi_dict[trial_label[batch]][entry] = np.append(isi_dict[trial_label[batch]][entry], isi_list[entry], axis=1)\n",
    "            # print('len dict after: ', len(isi_dict[trial_label[batch]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ISIs from paper\n",
    "norm_count = True\n",
    "norm_time = True\n",
    "# data_types = ['', '_noisy', '_temp_jitter', '_noisy_temp_jitter']\n",
    "filename = 'data_encoding_fix_len_noisy_temp_jitter'\n",
    "infile = open(f\"{data_path_original}/{filename}.pkl\", 'rb')\n",
    "data_original = pickle.load(infile)\n",
    "infile.close()\n",
    "isi_original = return_isi_fix_len(\n",
    "    data_original, max_trials, norm_count=norm_count, norm_time=norm_time)\n",
    "\n",
    "for _, entry in enumerate(isi_dict):\n",
    "    # iterate over channel\n",
    "    for channel in range(len(isi_dict[entry])):\n",
    "        # print('ISI: ', isi_dict[entry][channel][0])\n",
    "        # print('Count: ', isi_dict[entry][channel][1])\n",
    "        # print('Unique and count: ', np.unique(isi_dict[entry][channel][0], return_index=True))\n",
    "        isi, count = np.unique(isi_dict[entry][channel][0], return_index=True)\n",
    "        # only plot if spikes found in channel\n",
    "        if len(isi) > 0:\n",
    "            if norm_time:\n",
    "                isi = isi/max(isi)\n",
    "            if norm_count:\n",
    "                count = count/max(count)\n",
    "\n",
    "            figname = f'comparison ISI {entry} {channel}'\n",
    "            plt.figure(figsize=(12, 12))\n",
    "            plt.suptitle(figname)\n",
    "            # compare to original traces\n",
    "            for num, isi_original_sel in enumerate(isi_original):\n",
    "                plt.subplot(5, 4, num+1)\n",
    "                # if len(isi) > 1:\n",
    "                if norm_time:\n",
    "                    plt.bar(isi_original_sel[0], isi_original_sel[1], width=0.01)\n",
    "                    plt.bar(isi, count, width=0.01)\n",
    "                else:\n",
    "                    plt.bar(isi_original_sel[0], isi_original_sel[1])\n",
    "                    plt.bar(isi, count)\n",
    "                plt.xlim((0, 1.1))\n",
    "                # else:\n",
    "                #     plt.text(0.3, 0.5, 'Nope')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'{plot_out}/comparison_class_{entry}_channel_{channel}.png', dpi=300)\n",
    "            plt.close()\n",
    "            # plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\">Creative Commons Attribution 4.0 International License</a>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
