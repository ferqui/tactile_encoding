{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizer\n",
    "### Use this script to visualize the neuron traces according to the paper \"A Generalized Linear Integrate-and-Fire Neural Model Produces Diverse Spiking Behaviors\" by Stefan Mihalas and Ernst Niebur. Further, data was created with a fix length of 1sec (1ms time steps), with noise on the input current, and/or temporal jitter on the time point of the step for dynamic inputs. \n",
    "\n",
    "### The script will also calculate the inter-spike intervalls (ISIs) for a single trial and for all repeating trials, whenever possible. For repeating trials, all ISIs are grouped and further statics represent the outcome of all repetitions per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import os\n",
    "import progressbar\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.polynomial.polynomial as poly\n",
    "\n",
    "from tactile_encoding.utils.utils import value2key, create_directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nneuron parameters:\\na: 2.743\\nA1: 0.03712\\nA2: -0.5089\\nb: 11.4\\nG: 47.02\\nk1: 200\\nk2: 20\\nR1: 0\\nR2: 1\\n'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data_path_braille = './data/braille_mn_output'  # path to output from Braille data\n",
    "data_path_original = './data/original_mn_output'  # path to output from MN paper\n",
    "plot_out = './plots/braille'  # path to save plots\n",
    "create_directory(plot_out)\n",
    "data_types = ['', '_noisy', '_temp_jitter', '_offset', '_noisy_temp_jitter',\n",
    "              '_noisy_offset', '_temp_jitter_offset', '_noisy_temp_jitter_offset']\n",
    "\n",
    "braille_letters = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L',\n",
    "                   'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'Space']\n",
    "\n",
    "max_trials = 100\n",
    "\n",
    "classes_list = {\n",
    "    'A': \"Tonic spiking\",\n",
    "    'B': \"Class 1\",\n",
    "    'C': \"Spike frequency adaptation\",\n",
    "    'D': \"Phasic spiking\",\n",
    "    'E': \"Accommodation\",\n",
    "    'F': \"Threshold variability\",\n",
    "    'G': \"Rebound spike\",\n",
    "    'H': \"Class 2\",\n",
    "    'I': \"Integrator\",\n",
    "    'J': \"Input bistability\",\n",
    "    'K': \"Hyperpolarizing spiking\",\n",
    "    'L': \"Hyperpolarizing bursting\",\n",
    "    'M': \"Tonic bursting\",\n",
    "    'N': \"Phasic bursting\",\n",
    "    'O': \"Rebound burst\",\n",
    "    'P': \"Mixed mode\",\n",
    "    'Q': \"Afterpotentials\",\n",
    "    'R': \"Basal bistability\",\n",
    "    'S': \"Preferred frequency\",\n",
    "    'T': \"Spike latency\",\n",
    "}\n",
    "\n",
    "'''\n",
    "neuron parameters:\n",
    "a: 2.743\n",
    "A1: 0.03712\n",
    "A2: -0.5089\n",
    "b: 11.4\n",
    "G: 47.02\n",
    "k1: 200\n",
    "k2: 20\n",
    "R1: 0\n",
    "R2: 1\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_isi_fix_len(data, max_trials, norm_count=False, norm_time=False):\n",
    "    \"\"\"\n",
    "    Calculates and returns the ISI for all repetitions of fix length data.\n",
    "    \"\"\"\n",
    "    isi_list = []\n",
    "    for num, el in enumerate(list(classes_list.values())):\n",
    "        # print(el)\n",
    "        # concatenate all ISIs\n",
    "        isi_fix_len = []\n",
    "        for trial in range(max_trials):\n",
    "            # calc spikes per trial\n",
    "            spikes = np.reshape(np.array(data[trial + num*max_trials][0]), (np.array(\n",
    "                data[trial + num*max_trials][0]).shape[0]))\n",
    "            # calc ISI\n",
    "            isi_fix_len.extend(np.diff(np.where(spikes == 1)[0]))\n",
    "\n",
    "        if len(isi_fix_len) > 0:\n",
    "            tmp_fix_len = np.unique(isi_fix_len, return_counts=True)\n",
    "            isi_fix_len = tmp_fix_len[0]\n",
    "            if norm_time:\n",
    "                isi_fix_len = isi_fix_len/max(isi_fix_len)\n",
    "            isi_fix_len_count = tmp_fix_len[1]\n",
    "            if norm_count:\n",
    "                isi_fix_len_count = isi_fix_len_count/max(isi_fix_len_count)\n",
    "            # create 2d array\n",
    "            isi = np.vstack([isi_fix_len, isi_fix_len_count])\n",
    "        else:\n",
    "            isi = [[], []]\n",
    "        isi_list.append(isi)\n",
    "\n",
    "    return isi_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "### extract and concatenate all ISIs for each sensor over all data after training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ISIs from paper\n",
    "norm_count = True\n",
    "norm_time = True\n",
    "# for possible datatypes look at the top\n",
    "filename = 'data_encoding_fix_len_noisy_temp_jitter_offset'\n",
    "infile = open(f\"{data_path_original}/{filename}.pkl\", 'rb')\n",
    "data_original = pickle.load(infile)\n",
    "infile.close()\n",
    "isi_original = return_isi_fix_len(\n",
    "    data_original, max_trials, norm_count=norm_count, norm_time=norm_time)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare to paper output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per channel over all classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = os.listdir(data_path_braille)\n",
    "file_names = np.sort(file_names)\n",
    "init = True\n",
    "\n",
    "for _, file_name in enumerate(file_names):\n",
    "    [mn_spk, input_current, trial_label] = torch.load(\n",
    "        data_path_braille + '/' + file_name, map_location=torch.device('cpu'))\n",
    "    # convert to numpy\n",
    "    mn_spk = mn_spk.numpy()\n",
    "    input_current = input_current.numpy()\n",
    "    trial_label = trial_label.numpy()\n",
    "\n",
    "    # extract traces from single batch\n",
    "    for batch in range(mn_spk.shape[0]):\n",
    "        isi_list_channel = []\n",
    "\n",
    "        # loop over all channels\n",
    "        for channel in range(mn_spk[batch].shape[-1]):\n",
    "            # calc ISI per channel and append to list\n",
    "            isi_list_channel.append(\n",
    "                np.diff(np.where(mn_spk[batch][:, channel] == 1.0))*1E-2)\n",
    "\n",
    "        if init:\n",
    "            # init at first run\n",
    "            isi_list = isi_list_channel\n",
    "            init = False\n",
    "        else:\n",
    "            # extend with ISIs found in channel per batch\n",
    "            for num, _ in enumerate(isi_list_channel):\n",
    "                # print(len(isi_list[num][0]), len(isi_list_channel[num][0]))\n",
    "                isi_list[num] = np.append(\n",
    "                    isi_list[num], isi_list_channel[num], axis=1)\n",
    "                # print(len(isi_list[num][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uncertainty (x, y): [4.72455591 4.17475406]\n",
      "uncertainty (x, y): [0.60624505 0.15275062]\n",
      "uncertainty (x, y): [1.53472869 1.16421311]\n",
      "uncertainty (x, y): [0.7899664  0.48980438]\n",
      "uncertainty (x, y): [0.2948885  0.20444617]\n",
      "uncertainty (x, y): [0.98821409 0.88232867]\n",
      "Only got  0  ISIs.\n",
      "uncertainty (x, y): [1.53472869 1.16421311]\n",
      "uncertainty (x, y): [98.64139598 97.93684189]\n",
      "uncertainty (x, y): [0.37691653 0.20958568]\n",
      "Only got  0  ISIs.\n",
      "Only got  0  ISIs.\n",
      "uncertainty (x, y): [0.42947851 0.31198176]\n",
      "uncertainty (x, y): [2.52982213 1.94935887]\n",
      "Only got  0  ISIs.\n",
      "uncertainty (x, y): [0.45528963 0.26470891]\n",
      "uncertainty (x, y): [0.73092075 0.4402668 ]\n",
      "uncertainty (x, y): [0.51965944 0.40025107]\n",
      "uncertainty (x, y): [0.31299237 0.20686541]\n",
      "uncertainty (x, y): [0.49076663 0.25947159]\n"
     ]
    }
   ],
   "source": [
    "# linear fit on original data\n",
    "lin_fit_original = []\n",
    "cov_matr_original = []\n",
    "for num, isi_original_sel in enumerate(isi_original):\n",
    "    if len(isi_original_sel[0]) > 0:\n",
    "        # linear fit on original ISIs\n",
    "        [slope, offset], cov_matr = np.polyfit(isi_original_sel[0], isi_original_sel[1], 1, cov='unscaled')\n",
    "        print('uncertainty (x, y):', np.sqrt(np.diag(cov_matr)))\n",
    "        lin_fit_original.append([slope, offset])\n",
    "        cov_matr_original.append(cov_matr)\n",
    "    else:\n",
    "        print('Only got ', len(isi_original_sel[0]), ' ISIs.')\n",
    "        lin_fit_original.append([[], []])\n",
    "        cov_matr_original.append([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\n"
     ]
    }
   ],
   "source": [
    "# load ISIs from paper\n",
    "norm_count = True\n",
    "norm_time = True\n",
    "# data_types = ['', '_noisy', '_temp_jitter', '_noisy_temp_jitter_offset']\n",
    "filename = 'data_encoding_fix_len_noisy_temp_jitter_offset'\n",
    "infile = open(f\"{data_path_original}/{filename}.pkl\", 'rb')\n",
    "data_original = pickle.load(infile)\n",
    "infile.close()\n",
    "isi_original = return_isi_fix_len(\n",
    "    data_original, max_trials, norm_count=norm_count, norm_time=norm_time)\n",
    "error_list = []\n",
    "\n",
    "bar = progressbar.ProgressBar(maxval=len(isi_list)*len(isi_original), \\\n",
    "    widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
    "bar.start()\n",
    "\n",
    "# TODO save error values!\n",
    "# iterate channel (sensors)\n",
    "for channel, entry in enumerate(isi_list):\n",
    "    # extract single ISIs and their count\n",
    "    isi, count = np.unique(entry[0], return_index=True)\n",
    "\n",
    "    # only plot if ISIs found in channel\n",
    "    if len(isi) > 0:\n",
    "        if norm_time:\n",
    "            isi = isi/max(isi)\n",
    "        if norm_count:\n",
    "            count = count/max(count)\n",
    "\n",
    "        # linear fit on ISIs from channel (sensor)\n",
    "        slope_braille, offset_braille = np.polyfit(isi, count, 1)\n",
    "        # https://numpy.org/doc/stable/reference/generated/numpy.polyfit.html\n",
    "        [slope_braille, offset_braille], cov_matr_braille = np.polyfit(isi, count, 1, cov='unscaled')\n",
    "        # print('uncertainty (x, y): ', np.sqrt(np.diag(cov_matr_braille)))\n",
    "        # create box plots with all classes from paper and single sensor and class for Braille data\n",
    "        figname = f'comparison ISI sensor: {channel}'\n",
    "        plt.figure(figname, figsize=(12, 12))\n",
    "        plt.suptitle(figname)\n",
    "\n",
    "        error = []\n",
    "        # compare to original traces\n",
    "        for num, isi_original_sel in enumerate(isi_original):\n",
    "            # create scatter plot for Braille and original ISIs\n",
    "            plt.subplot(5, 4, num+1)\n",
    "            plt.title(f'{classes_list[braille_letters[num]]}')\n",
    "            if len(isi_original_sel[0]) > 0:\n",
    "                # load linear fit for original\n",
    "                slope_original, offset_original = lin_fit_original[num]\n",
    "                # TODO inlcude check of uncertainty. \n",
    "                # High uncertainty -> bad fit -> bad data representation -> not reliable\n",
    "                # find threshold\n",
    "\n",
    "                # slope is a good first indicator some similarity\n",
    "                if np.sign(slope_original) == np.sign(slope_braille):\n",
    "                    # TODO error = max(error). No sqrt needed\n",
    "                    # calc error between line fits\n",
    "                    line_braille = slope_braille*np.linspace(0.0, 1.0, 10)+offset_braille\n",
    "                    line_original = slope_original*np.linspace(0.0, 1.0, 10)+offset_original\n",
    "                    error_of_fits = np.sqrt(np.mean((line_braille-line_original)**2))\n",
    "                    error.append(error_of_fits)\n",
    "                    # print('error: ', error_of_fits)\n",
    "                else:\n",
    "                    error.append([])\n",
    "                    # print('Skipped error computation.')\n",
    "            \n",
    "                plt.scatter(isi_original_sel[0], isi_original_sel[1], color='tab:blue')\n",
    "                plt.plot(np.linspace(0.0, 1.0, 10), slope_original*np.linspace(0.0, 1.0, 10)+offset_original, color='tab:blue') \n",
    "            else:\n",
    "                error.append([])\n",
    "                plt.text(0.3, 0.5, f'nbr. ISIs = {len(isi_original_sel[0])}')\n",
    "            # https://matplotlib.org/stable/gallery/color/named_colors.html\n",
    "            plt.scatter(isi, count, color='tab:orange')\n",
    "            plt.plot(np.linspace(0.0, 1.0, 10), slope_braille*np.linspace(0.0, 1.0, 10)+offset_braille, color='tab:orange') \n",
    "            plt.xlim((0, 1.1))\n",
    "            plt.ylim((0, 1.1))\n",
    "            if num == 0 or num == 4 or num == 8 or num == 12 or num == 16:\n",
    "                plt.ylabel('Count')\n",
    "            if num > 15:\n",
    "                plt.xlabel('ISI')\n",
    "            bar.update(channel*len(isi_original)+num)\n",
    "\n",
    "        error_list.append(error)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\n",
    "            f'{plot_out}/comparison_all_classes_channel_{channel}_scatter.png', dpi=300)\n",
    "        plt.close()\n",
    "    else:\n",
    "        bar.update(channel*len(isi_original)+len(isi_original))\n",
    "bar.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\n"
     ]
    }
   ],
   "source": [
    "# load ISIs from paper\n",
    "norm_count = True\n",
    "norm_time = True\n",
    "# data_types = ['', '_noisy', '_temp_jitter', '_noisy_temp_jitter_offset']\n",
    "filename = 'data_encoding_fix_len_noisy_temp_jitter_offset'\n",
    "infile = open(f\"{data_path_original}/{filename}.pkl\", 'rb')\n",
    "data_original = pickle.load(infile)\n",
    "infile.close()\n",
    "isi_original = return_isi_fix_len(\n",
    "    data_original, max_trials, norm_count=norm_count, norm_time=norm_time)\n",
    "\n",
    "bar = progressbar.ProgressBar(maxval=len(isi_list)*len(isi_original), \\\n",
    "    widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
    "bar.start()\n",
    "\n",
    "# iterate channel (sensors)\n",
    "for channel, entry in enumerate(isi_list):\n",
    "    # extract single ISIs and their count\n",
    "    isi, count = np.unique(entry[0], return_index=True)\n",
    "\n",
    "    # only plot if ISIs found in channel\n",
    "    if len(isi) > 0:\n",
    "        if norm_time:\n",
    "            isi = isi/max(isi)\n",
    "        if norm_count:\n",
    "            count = count/max(count)\n",
    "\n",
    "        # create box plots with all classes from paper and single sensor and class for Braille data\n",
    "        figname = f'comparison ISI sensor: {channel}'\n",
    "        plt.figure(figname, figsize=(12, 12))\n",
    "        plt.suptitle(figname)\n",
    "\n",
    "        # compare to original traces\n",
    "        for num, isi_original_sel in enumerate(isi_original):\n",
    "            # create scatter plot for Braille and original ISIs\n",
    "            plt.subplot(5, 4, num+1)\n",
    "            plt.title(f'{classes_list[braille_letters[num]]}')\n",
    "            if len(isi_original_sel[0]) > 0:\n",
    "                if norm_time:\n",
    "                    plt.bar(isi_original_sel[0], isi_original_sel[1], width=0.01, color='tab:blue')\n",
    "                else:\n",
    "                    plt.bar(isi_original_sel[0], isi_original_sel[1], color='tab:blue')\n",
    "            else:\n",
    "                plt.text(0.3, 0.5, f'nbr. ISIs = {len(isi_original_sel[0])}')\n",
    "            if norm_time:\n",
    "                plt.bar(isi, count, width=0.01, color='tab:orange')\n",
    "            else:\n",
    "                plt.bar(isi, count, color='tab:orange')\n",
    "            plt.xlim((0, 1.1))\n",
    "            if num == 0 or num == 4 or num == 8 or num == 12 or num == 16:\n",
    "                plt.ylabel('Count')\n",
    "            if num > 15:\n",
    "                plt.xlabel('ISI')\n",
    "            bar.update(channel*len(isi_original)+num)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\n",
    "            f'{plot_out}/comparison_all_classes_channel_{channel}_bar.png', dpi=300)\n",
    "        plt.close()\n",
    "    else:\n",
    "        bar.update(channel*len(isi_original)+len(isi_original))\n",
    "bar.finish()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find temporal evolution of predicted classes\n",
    "### Given we have the output spike trains from the trained classifier for MN original classes, we can use a sliding window to determine the evolution of the networks prediction over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m data_path_original_classifier \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m./data/original_classifier\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m file_names \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mlistdir(data_path_original_classifier)\n\u001b[1;32m      3\u001b[0m file_names \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msort(file_names)\n\u001b[1;32m      4\u001b[0m init \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "data_path_original_classifier = './data/original_classifier'\n",
    "file_names = os.listdir(data_path_original_classifier)\n",
    "file_names = np.sort(file_names)\n",
    "init = True\n",
    "window_size = 50  # ms\n",
    "\n",
    "for _, file_name in enumerate(file_names):\n",
    "    original_classifier_spk = torch.load(\n",
    "        data_path_braille + '/' + file_name, map_location=torch.device('cpu'))\n",
    "    # convert to numpy\n",
    "    original_classifier_spk = original_classifier_spk.numpy()\n",
    "\n",
    "    # extract traces from single batch\n",
    "    for batch in range(mn_spk.shape[0]):\n",
    "        prediction_list_channel = []\n",
    "\n",
    "        # loop over all channels\n",
    "        for channel in range(original_classifier_spk[batch].shape[-1]):\n",
    "            # loop over temporal increments\n",
    "            for window in range(len(original_classifier_spk[batch][:, channel]), window_size):\n",
    "                # calc winning class in time window\n",
    "                prediction_list_channel.append(np.sum(original_classifier_spk[batch][window_size*window:window_size*(window+1), channel], axis=0))\n",
    "\n",
    "        if init:\n",
    "            # init at first run\n",
    "            prediction_list = prediction_list_channel\n",
    "            init = False\n",
    "        else:\n",
    "            # extend with ISIs found in channel per batch\n",
    "            for num, _ in enumerate(prediction_list_channel):\n",
    "                # print(len(isi_list[num][0]), len(isi_list_channel[num][0]))\n",
    "                prediction_list[num] = np.append(\n",
    "                    prediction_list[num], prediction_list_channel[num], axis=1)\n",
    "                # print(len(isi_list[num][0]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\">Creative Commons Attribution 4.0 International License</a>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
