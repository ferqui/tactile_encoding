{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom function(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Designed to work with all the activity datasets collected from \n",
    "# Braille and MNIST classification for the NTE-Encoding project\n",
    "def load_activity(path, subset, stimulus_labels=None):\n",
    "\n",
    "    \"\"\"\n",
    "    To load the dataset you need to\n",
    "    - load the spikes using np.load() and then run np.unpackbits(axis=0)\n",
    "    - load the labels using torch.load()\n",
    "\n",
    "    stimulus_labels can be used for 'additional' labelling, as in the case \n",
    "    of the Braille dataset, to keep the match between the actual labels and\n",
    "    their numeric representation\n",
    "\n",
    "    Fra, Vittorio,\n",
    "    Politecnico di Torino,\n",
    "    EDA Group,\n",
    "    Torino, Italy.\n",
    "    \"\"\"\n",
    "    \n",
    "    names_listdir = os.listdir(os.path.join(path,subset))\n",
    "    spikes_file_list = [ii for ii in names_listdir if ii.endswith(\".npy\")]\n",
    "    #max_file_idx = np.max([int(ii.rstrip(\".npy\").split(\"_\")[-1]) for ii in spikes_file_list])\n",
    "    #common_prefix = next((spikes_file_list[0][:i] for i,(p,*r) in enumerate(zip(*spikes_file_list)) if any(p!=c for c in r)),min(spikes_file_list,key=len))\n",
    "\n",
    "    spikes = []\n",
    "    labels = []\n",
    "\n",
    "    for ii in spikes_file_list:\n",
    "        \n",
    "        lbl = torch.load(os.path.join(path,subset,ii.rstrip(\".npy\")+\"_label.pt\"))\n",
    "        spk = torch.swapdims(torch.as_tensor(np.unpackbits(np.load(os.path.join(path,subset,ii)),axis=0,count=lbl.shape[0]), dtype=torch.float32),1,2)\n",
    "        lbl = lbl.tile(spk.shape[1])\n",
    "        spk = spk.reshape(-1,spk.shape[-1])\n",
    "        labels.append(lbl)\n",
    "        spikes.append(spk)\n",
    "    \n",
    "    overall_label = torch.cat(labels)\n",
    "    individual_channels_activity = torch.cat(spikes)\n",
    "\n",
    "    activity_df = pd.DataFrame()\n",
    "    activity_df[\"Activity\"] = list(individual_channels_activity)\n",
    "    activity_df[\"Label\"] = list(overall_label)\n",
    "    if stimulus_labels != None:\n",
    "        overall_stimulus = []\n",
    "        for ii in overall_label:\n",
    "            overall_stimulus.append(stimulus_labels[ii])\n",
    "        activity_df[\"Stimulus\"] = list(overall_stimulus)\n",
    "\n",
    "    return activity_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity split analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Braille\n",
    "stimulus_labels = ['Space', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K',\n",
    "    'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
    "root_actdir = \"./MN_output_Braille/\"\n",
    "common_prefix = \"GR_braille\"\n",
    "\n",
    "# # For MNIST\n",
    "# stimulus_labels = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "# root_actdir = \"./MN_Output_MNIST/\"\n",
    "# common_prefix = \"GR_mnist\"\n",
    "# #root_actdir = \"./Activity/MN_Output_MNIST_c/\" # (compressed)\n",
    "# #common_prefix = \"GR_mnist_compressed\" # (compressed)\n",
    "\n",
    "suffix = [\"w\"]\n",
    "subsets = [\"train\", \"eval\", \"test\"]\n",
    "\n",
    "for sfx in suffix:\n",
    "\n",
    "    seed_folders = [ii for ii in os.listdir(root_actdir) if f\"{common_prefix}_{sfx}\" in ii]\n",
    "    max_seed = np.max([int(fld.split(\"_\")[-1]) for fld in seed_folders])\n",
    "\n",
    "    for seed in range(max_seed+1):\n",
    "\n",
    "        seed_folder = f\"{common_prefix}_{sfx}_{seed}\"\n",
    "\n",
    "        if seed_folder in seed_folders:\n",
    "\n",
    "            for subset in subsets:\n",
    "\n",
    "                files = os.listdir(os.path.join(root_actdir,seed_folder,subset))\n",
    "                files_lbl = [f for f in files if f.endswith(\".pt\")]\n",
    "                files_val = [f for f in files if f.endswith(\".npy\")]\n",
    "                labels = []\n",
    "                values = []\n",
    "                for file in files_lbl:\n",
    "                    labels.append(torch.load(os.path.join(os.path.join(root_actdir,seed_folder,subset,file))))\n",
    "                for file in files_val:\n",
    "                    values.extend(np.unpackbits(np.load(os.path.join(root_actdir,seed_folder,subset,file)),axis=0))\n",
    "                labels = torch.cat(labels)\n",
    "                values = np.array(values)\n",
    "\n",
    "                print(f\"Data from {os.path.join(seed_folder,subset)}:\")\n",
    "                print(\"\\tLabels:\")\n",
    "                print(f\"\\t\\tshape: {labels.shape}\")\n",
    "                unique_lbl, count_lbl = np.unique(labels, return_counts=True)\n",
    "                if len(np.unique(count_lbl)) > 1:\n",
    "                    unique_lbl_str = []\n",
    "                    for num,el in enumerate(unique_lbl):\n",
    "                        unique_lbl_str.append(stimulus_labels[el])\n",
    "                    print(f\"\\t\\tcount per label: {dict(zip(unique_lbl_str, count_lbl))}\")\n",
    "                else:\n",
    "                    count, _ = np.unique(count_lbl, return_counts=True)\n",
    "                    print(f\"\\t\\tcount per label: {count.item()}\")\n",
    "\n",
    "                print(\"\\tValues:\")\n",
    "                print(f\"\\t\\tshape: {values.shape}\")\n",
    "\n",
    "                spike_count, occurrences = np.unique(np.count_nonzero(values, axis=1), return_counts=True)\n",
    "                plt.figure()\n",
    "                plt.scatter(spike_count, occurrences/(values.shape[0]*values.shape[-1])*100, s=12, c=\"tab:red\", zorder=3)\n",
    "                plt.xlabel(f\"Spike count [over the {values.shape[1]} time steps]\")\n",
    "                plt.ylabel(\"Occurrences (%)\")\n",
    "                plt.xlim(-10,values.shape[1]*1.02)\n",
    "                #plt.ylim(0.5, 1.01*len(labels)*6)\n",
    "                plt.ylim(1e-5, 200)\n",
    "                plt.yscale(\"log\")\n",
    "                plt.grid(visible=True, which='both', axis='y', zorder=1)\n",
    "                plt.title(os.path.join(seed_folder,subset))\n",
    "                plt.show()\n",
    "                \n",
    "            print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the activity data and create a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For Braille\n",
    "# path = \"./Activity/MN_output_Braille/GR_braille_w_0\"\n",
    "# subset = \"test\"\n",
    "# stimulus_labels = ['Space', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K',\n",
    "#     'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
    "\n",
    "# For MNIST\n",
    "path = \"./Activity/MN_Output_MNIST/GR_mnist_w_0\"\n",
    "subset = \"test\"\n",
    "stimulus_labels = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(os.path.join(path,subset))\n",
    "files_lbl = [f for f in files if f.endswith(\".pt\")]\n",
    "files_val = [f for f in files if f.endswith(\".npy\")]\n",
    "labels = []\n",
    "values = []\n",
    "for file in files_lbl:\n",
    "    labels.append(torch.load(os.path.join(os.path.join(path,subset,file))))\n",
    "for file in files_val:\n",
    "    values.extend(np.unpackbits(np.load(os.path.join(path,subset,file)),axis=0))\n",
    "labels = torch.cat(labels)\n",
    "values = np.array(values)\n",
    "\n",
    "print(f\"Subset: {subset}\")\n",
    "print(\"\\tLabels:\")\n",
    "print(f\"\\t\\tshape: {labels.shape}\")\n",
    "unique_lbl, count_lbl = np.unique(labels, return_counts=True)\n",
    "unique_lbl_str = []\n",
    "for num,el in enumerate(unique_lbl):\n",
    "    unique_lbl_str.append(stimulus_labels[el])\n",
    "print(f\"\\t\\tcount per label: {dict(zip(unique_lbl_str, count_lbl))}\")\n",
    "\n",
    "print(\"\\tValues:\")\n",
    "print(f\"\\t\\tshape: {values.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_activity(path, subset, stimulus_labels)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Data from: \\n\\t{os.path.join(path,subset)}\")\n",
    "unique_lbl, count_lbl = np.unique(df[\"Stimulus\"], return_counts=True)\n",
    "print(f\"Number of samples: \\n\\t{int(df.shape[0]/values.shape[-1])}\")\n",
    "if int(df.shape[0]/values.shape[-1]/len(stimulus_labels)) == df.shape[0]/values.shape[-1]/len(stimulus_labels):\n",
    "    print(f\"Number of repetitions for each class sample: \\n\\t{int(df.shape[0]/values.shape[-1]/len(stimulus_labels))}\")\n",
    "else:\n",
    "    print(f\"Number of repetitions for each class sample: \\n\\t{df.shape[0]/values.shape[-1]/len(stimulus_labels)}\")\n",
    "print(f\"Number of single channel recordings: \\n\\t{df.shape[0]}\")\n",
    "if len(np.unique(count_lbl)) == 1:\n",
    "    count, _ = np.unique(count_lbl, return_counts=True)\n",
    "    print(f\"Number of single channel recordings for each class sample: \\n\\t{count.item()}\")\n",
    "else:\n",
    "    print(f\"Number of single channel recordings for each class sample: \\n\\t{dict(zip(unique_lbl, count_lbl))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Read the pickle from activity classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Letter</th>\n",
       "      <th>Behaviour</th>\n",
       "      <th>Probabilities</th>\n",
       "      <th>Spikes</th>\n",
       "      <th>Sparsity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>[[0.0, 0.0, 11.42, 0.0, 0.0, 0.0, 0.0, 0.0, 0....</td>\n",
       "      <td>59</td>\n",
       "      <td>0.8127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S</td>\n",
       "      <td>O</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>34</td>\n",
       "      <td>0.8921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>J</td>\n",
       "      <td>G</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 100.0, 0.0, 0....</td>\n",
       "      <td>2</td>\n",
       "      <td>0.9937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Y</td>\n",
       "      <td>O</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>41</td>\n",
       "      <td>0.8698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>W</td>\n",
       "      <td>A</td>\n",
       "      <td>[[24.69, 0.0, 24.69, 0.0, 0.0, 0.0, 24.69, 24....</td>\n",
       "      <td>107</td>\n",
       "      <td>0.6603</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Letter Behaviour                                      Probabilities  Spikes  \\\n",
       "0      G         O  [[0.0, 0.0, 11.42, 0.0, 0.0, 0.0, 0.0, 0.0, 0....      59   \n",
       "1      S         O  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...      34   \n",
       "2      J         G  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 100.0, 0.0, 0....       2   \n",
       "3      Y         O  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...      41   \n",
       "4      W         A  [[24.69, 0.0, 24.69, 0.0, 0.0, 0.0, 24.69, 24....     107   \n",
       "\n",
       "   Sparsity  \n",
       "0    0.8127  \n",
       "1    0.8921  \n",
       "2    0.9937  \n",
       "3    0.8698  \n",
       "4    0.6603  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act_clas = pd.read_pickle(\"../../results/activity_classification/MN_activity/MN_output_Braille/GR_braille_w_0_train_20240116_105004.pkl\")\n",
    "act_clas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of active channels found: 38334\n",
      "\n",
      "Number of active channels found for each Braille character:\n",
      "{'A': 1416, 'B': 1526, 'C': 1312, 'D': 1435, 'E': 1315, 'F': 1582, 'G': 1389, 'H': 1475, 'I': 1342, 'J': 1406, 'K': 1506, 'L': 1329, 'M': 1599, 'N': 1360, 'O': 1429, 'P': 1340, 'Q': 1419, 'R': 1519, 'S': 1330, 'Space': 1356, 'T': 1375, 'U': 1536, 'V': 1360, 'W': 1588, 'X': 1454, 'Y': 1271, 'Z': 1365}\n"
     ]
    }
   ],
   "source": [
    "chars, counts = np.unique(act_clas[\"Letter\"], return_counts=True)\n",
    "\n",
    "print(f\"Total number of active channels found: {len(act_clas)}\\n\")\n",
    "\n",
    "print(\"Number of active channels found for each Braille character:\")\n",
    "print(dict(zip(chars,counts)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('env_nteEnc': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "77798a54e7665e7cdc06a08ee54c1994981fcf46c4af6330a97b3c21e089a8c7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
